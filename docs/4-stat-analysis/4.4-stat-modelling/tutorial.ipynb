{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Statistical Modeling Tutorial\n",
    "\n",
    "This notebook covers key concepts in statistical modeling including:\n",
    "- Model Selection\n",
    "- Polynomial Regression\n",
    "- Logistic Regression\n",
    "- Regularization\n",
    "- Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Selection\n",
    "\n",
    "Let's demonstrate how to select appropriate models based on data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate non-linear data\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = 0.5 * X.ravel()**3 - X.ravel()**2 + 2 + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compare different polynomial degrees\n",
    "degrees = [1, 2, 3, 5]\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, degree in enumerate(degrees, 1):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly_train = poly.fit_transform(X_train)\n",
    "    X_poly_test = poly.transform(X_test)\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_poly_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.scatter(X_test, y_test, color='blue', alpha=0.5, label='Actual')\n",
    "    \n",
    "    # Sort for smooth curve plotting\n",
    "    X_plot = np.sort(X_test, axis=0)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "    \n",
    "    plt.plot(X_plot, y_plot, color='red', label=f'Degree {degree}')\n",
    "    plt.title(f'Polynomial (d={degree})\\nMSE: {mse:.2f}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Polynomial Regression\n",
    "\n",
    "Let's explore polynomial regression in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create polynomial regression pipeline\n",
    "def create_poly_pipeline(degree):\n",
    "    return Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "\n",
    "# Fit models and evaluate using cross-validation\n",
    "degrees = range(1, 8)\n",
    "cv_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    pipeline = create_poly_pipeline(degree)\n",
    "    scores = -cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, cv_scores, marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error (CV)')\n",
    "plt.title('Model Complexity vs. Error')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print best degree\n",
    "best_degree = degrees[np.argmin(cv_scores)]\n",
    "print(f\"Best polynomial degree: {best_degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression\n",
    "\n",
    "Let's implement logistic regression for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate classification data\n",
    "n_samples = 200\n",
    "X_class = np.random.randn(n_samples, 2)\n",
    "y_class = (X_class[:, 0] + X_class[:, 1] > 0).astype(int)\n",
    "\n",
    "# Split data\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit logistic regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_class = log_reg.predict(X_test_class)\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(X, y, model):\n",
    "    h = 0.02  # step size in mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_decision_boundary(X_class, y_class, log_reg)\n",
    "plt.title(f'Logistic Regression Decision Boundary\\nAccuracy: {accuracy:.2f}')\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularization\n",
    "\n",
    "Let's compare different regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate data with many features\n",
    "n_samples, n_features = 100, 20\n",
    "X_reg = np.random.randn(n_samples, n_features)\n",
    "true_coefficients = np.zeros(n_features)\n",
    "true_coefficients[:5] = [1, 0.5, -0.8, 0.3, -0.4]  # Only first 5 features are relevant\n",
    "y_reg = np.dot(X_reg, true_coefficients) + np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Compare models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (L2)': Ridge(alpha=1.0),\n",
    "    'Lasso (L1)': Lasso(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Fit models and collect coefficients\n",
    "coefficients = {}\n",
    "test_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_reg, y_train_reg)\n",
    "    coefficients[name] = model.coef_\n",
    "    test_scores[name] = model.score(X_test_reg, y_test_reg)\n",
    "\n",
    "# Plot coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(n_features)\n",
    "\n",
    "for name, coef in coefficients.items():\n",
    "    plt.plot(x, coef, 'o-', label=f'{name} (RÂ² = {test_scores[name]:.3f})')\n",
    "\n",
    "plt.plot(x, true_coefficients, 'k--', label='True Coefficients')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Comparison of Regularization Methods')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation\n",
    "\n",
    "Let's explore techniques for interpreting our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance analysis\n",
    "def plot_feature_importance(model, feature_names=None):\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature {i}' for i in range(len(model.coef_))]\n",
    "    \n",
    "    importance = np.abs(model.coef_)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    })\n",
    "    \n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(importance)), feature_importance['Importance'])\n",
    "    plt.yticks(range(len(importance)), feature_importance['Feature'])\n",
    "    plt.xlabel('Absolute Coefficient Value')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot feature importance for each model\n",
    "for name, model in models.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_feature_importance(model)\n",
    "    plt.title(f'Feature Importance - {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Experiment with different regularization strengths (alpha values) and observe their effects.\n",
    "\n",
    "2. Implement cross-validation for hyperparameter tuning.\n",
    "\n",
    "3. Create a more complex classification problem and solve it using logistic regression.\n",
    "\n",
    "4. Compare polynomial regression with other non-linear modeling techniques.\n",
    "\n",
    "5. Develop a comprehensive model evaluation framework including multiple metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
