# Sampling Distributions: The Heart of Statistical Inference üìä

## Introduction: Why Sampling Distributions Matter üéØ
Imagine you're a chef trying to perfect a recipe. You taste-test small portions (samples) to understand how the entire dish (population) tastes. But how reliable are these taste tests? That's where sampling distributions come in - they help us understand how sample statistics vary and how well they represent the true population! üç≤

## What is a Sampling Distribution? üé≤
A sampling distribution is the distribution of a statistic (like mean or proportion) calculated from repeated random samples of the same size from a population. Think of it as the "distribution of distributions" - it shows us how sample statistics bounce around the true population value.

### Mathematical Definition
For a sample mean $\bar{X}$:
- Mean: $E(\bar{X}) = \mu$ (population mean)
- Standard Error: $SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}$
  - where $\sigma$ is population standard deviation
  - and $n$ is sample size

## The Central Limit Theorem (CLT): Statistical Magic! ‚ú®

### What is CLT?
The Central Limit Theorem states that for sufficiently large samples:
1. The sampling distribution of the mean is approximately normal
2. This holds true regardless of the population's distribution
3. The larger the sample size, the more normal it becomes

Let's see it in action!

\`\`\`python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def demonstrate_clt(distribution='exponential', sample_size=30, n_samples=1000):
    """
    Demonstrate CLT with different distributions
    """
    plt.figure(figsize=(15, 5))
    
    # Generate population
    if distribution == 'exponential':
        population = np.random.exponential(scale=1.0, size=10000)
        title = 'Exponential Distribution'
    elif distribution == 'uniform':
        population = np.random.uniform(0, 1, 10000)
        title = 'Uniform Distribution'
    else:  # Skewed custom distribution
        population = np.concatenate([
            np.random.normal(0, 1, 7000),
            np.random.normal(3, 0.5, 3000)
        ])
        title = 'Skewed Distribution'
    
    # Take many samples and calculate their means
    sample_means = [
        np.mean(np.random.choice(population, size=sample_size))
        for _ in range(n_samples)
    ]
    
    # Plot results
    plt.subplot(131)
    plt.hist(population, bins=50, density=True, alpha=0.7, color='skyblue')
    plt.title(f'Population Distribution\n({title})')
    plt.xlabel('Value')
    plt.ylabel('Density')
    
    plt.subplot(132)
    sample = np.random.choice(population, size=sample_size)
    plt.hist(sample, bins=20, density=True, alpha=0.7, color='lightgreen')
    plt.title(f'One Sample Distribution\n(n={sample_size})')
    plt.xlabel('Value')
    
    plt.subplot(133)
    plt.hist(sample_means, bins=30, density=True, alpha=0.7, color='salmon')
    x = np.linspace(min(sample_means), max(sample_means), 100)
    plt.plot(x, stats.norm.pdf(x, np.mean(sample_means), np.std(sample_means)),
             'k--', label='Normal Curve')
    plt.title(f'Sampling Distribution\nof the Mean')
    plt.xlabel('Sample Mean')
    plt.legend()
    
    plt.tight_layout()
    return plt

# Create and save plots for different distributions
distributions = ['exponential', 'uniform', 'skewed']
for dist in distributions:
    plt = demonstrate_clt(distribution=dist)
    plt.savefig(f'docs/4-stat-analysis/4.1-inferential-stats/assets/clt_{dist}.png')
    plt.close()
\`\`\`

## Standard Error: Measuring the Spread üìè

The standard error (SE) tells us how much sample statistics typically deviate from the population parameter. It's like a "margin of wobble" for our estimates!

### Formula for Different Statistics
1. For means: $SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}$
2. For proportions: $SE(p) = \sqrt{\frac{p(1-p)}{n}}$
3. For differences: $SE(\bar{X}_1 - \bar{X}_2) = \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$

Let's see how sample size affects SE:

\`\`\`python
def demonstrate_standard_error():
    """
    Show how SE changes with sample size
    """
    # Generate population
    np.random.seed(42)
    population = np.random.normal(100, 15, 10000)
    
    # Test different sample sizes
    sizes = [10, 30, 100, 300, 1000]
    results = []
    
    for n in sizes:
        # Theoretical SE
        theoretical_se = np.std(population) / np.sqrt(n)
        
        # Empirical SE (from sampling distribution)
        sample_means = [
            np.mean(np.random.choice(population, size=n))
            for _ in range(1000)
        ]
        empirical_se = np.std(sample_means)
        
        results.append({
            'size': n,
            'theoretical': theoretical_se,
            'empirical': empirical_se
        })
    
    return results

# Run demonstration
se_results = demonstrate_standard_error()
print("\nüéØ Standard Error Analysis")
print("Sample Size | Theoretical SE | Empirical SE")
print("-" * 45)
for r in se_results:
    print(f"{r['size']:^10d} | {r['theoretical']:^13.3f} | {r['empirical']:^11.3f}")
\`\`\`

## Real-world Applications üåç

### 1. Quality Control in Manufacturing üè≠
\`\`\`python
def quality_control_demo():
    """
    Simulate quality control in manufacturing
    """
    # Target specification: 100 ¬± 2 units
    target = 100
    tolerance = 2
    
    # Production line measurements (30 samples per hour)
    measurements = np.random.normal(100.5, 1.5, 30)
    mean = np.mean(measurements)
    se = stats.sem(measurements)
    
    print("\nüè≠ Quality Control Report")
    print(f"Specification: {target} ¬± {tolerance}")
    print(f"Sample Mean: {mean:.2f}")
    print(f"Standard Error: {se:.3f}")
    print(f"Status: {'‚úÖ In Control' if abs(mean - target) <= tolerance else '‚ùå Out of Control'}")

quality_control_demo()
\`\`\`

### 2. Political Polling üìä
\`\`\`python
def polling_demo():
    """
    Simulate political polling
    """
    # True population support: 52%
    true_support = 0.52
    sample_size = 1000
    
    # Simulate poll
    poll = np.random.binomial(1, true_support, sample_size)
    p_hat = np.mean(poll)
    se = np.sqrt(p_hat * (1-p_hat) / sample_size)
    
    print("\nüìä Political Poll Results")
    print(f"Support: {p_hat:.1%}")
    print(f"Margin of Error (95% CI): ¬±{1.96*se:.1%}")
    print(f"Sample Size: {sample_size:,}")

polling_demo()
\`\`\`

## Common Misconceptions: Let's Clear Them Up! ‚ö†Ô∏è

### 1. Sampling Distribution vs. Sample Distribution
- üìä Sample Distribution: The spread of values in ONE sample
- üé≤ Sampling Distribution: The spread of statistics from MANY samples

### 2. Standard Deviation vs. Standard Error
- üìè Standard Deviation: Spread of individual values
- üéØ Standard Error: Spread of sample statistics

### 3. Sample Size Effects
- ‚ùå "Larger samples always give the right answer"
- ‚úÖ "Larger samples give more precise estimates"

## Interactive Learning: Try It Yourself! ü§ì

### Mini-Exercise: The Sampling Game
\`\`\`python
def sampling_game(true_mean=100, true_std=15, sample_size=30):
    """
    Interactive demonstration of sampling variability
    """
    population = np.random.normal(true_mean, true_std, 10000)
    sample = np.random.choice(population, size=sample_size)
    sample_mean = np.mean(sample)
    se = np.std(sample) / np.sqrt(sample_size)
    
    print("\nüéÆ The Sampling Game")
    print(f"Sample Mean: {sample_mean:.1f}")
    print(f"Standard Error: {se:.2f}")
    print(f"95% CI: ({sample_mean - 1.96*se:.1f}, {sample_mean + 1.96*se:.1f})")
    print(f"Contains true mean? {'‚úÖ' if true_mean-1.96*se <= sample_mean <= true_mean+1.96*se else '‚ùå'}")

sampling_game()
\`\`\`

## Practice Questions üìù
1. A sample of 100 customers shows mean spending of $85 with SE=$5. What's the 95% CI?
2. How would doubling sample size affect the standard error? Show the math!
3. Why might the CLT not work well with very small samples?
4. Design a sampling strategy for estimating average daily website traffic.
5. How would you explain sampling distributions to a non-technical stakeholder?

## Key Takeaways üéØ
1. üìä Sampling distributions help us understand estimation uncertainty
2. üé≤ The CLT is the foundation of many statistical methods
3. üìè Larger samples give more precise estimates (smaller SE)
4. ‚öñÔ∏è There's always a trade-off between precision and cost
5. üéØ Understanding sampling distributions is crucial for inference

## Additional Resources üìö
- [Interactive CLT Simulator](https://seeing-theory.brown.edu/sampling-distributions/index.html)
- [Standard Error Calculator](https://www.calculator.net/standard-error-calculator.html)
- [Sampling Distribution Visualizer](https://rpsychologist.com/d3/CI/)

Remember: Sampling distributions are like a GPS for statistics - they help us navigate from sample to population with confidence! üó∫Ô∏è
