# Quiz: Model Evaluation

## Questions

1. What is k-fold cross-validation?
   _a. Splitting data into k training/validation sets_
   b. Training k different models
   c. Selecting k best features
   d. Testing k times

**Explanation**: K-fold cross-validation:
- Divides data into k equal parts
- Uses each part as validation set once
- Trains on remaining k-1 parts
- Averages performance across folds
Benefits:
- More robust performance estimates
- Uses all data for both training and validation
- Reduces overfitting risk
- Better generalization assessment

*For more information, see: [Cross Validation](../5.5-model-eval/cross-validation.md)*

2. What is grid search used for?
   _a. Finding optimal hyperparameters_
   b. Searching through data
   c. Finding best features
   d. Searching for patterns

**Explanation**: Grid search:
- Systematically works through hyperparameter combinations
- Evaluates each combination using cross-validation
- Finds best parameter settings
- Exhaustive search of specified parameter space
Implementation considerations:
- Parameter range specification
- Computational cost
- Cross-validation strategy
- Scoring metric choice

*For more information, see: [Hyperparameter Tuning](../5.5-model-eval/hyperparameter-tuning.md)*

3. What is a confusion matrix?
   _a. Table showing prediction results_
   b. Matrix of feature correlations
   c. Table of model parameters
   d. Matrix of error terms

**Explanation**: Confusion matrix shows:
- True Positives (TP)
- True Negatives (TN)
- False Positives (FP)
- False Negatives (FN)
Derived metrics:
- Accuracy: (TP + TN) / Total
- Precision: TP / (TP + FP)
- Recall: TP / (TP + FN)
- F1-Score: 2 × (Precision × Recall) / (Precision + Recall)

*For more information, see: [Cross Validation](../5.5-model-eval/cross-validation.md)*

4. What is ROC curve?
   _a. Plot of true vs false positive rates_
   b. Plot of training error
   c. Plot of model parameters
   d. Plot of feature importance

**Explanation**: ROC (Receiver Operating Characteristic) curve:
- Shows tradeoff between sensitivity and specificity
- Plots True Positive Rate vs False Positive Rate
- Area Under Curve (AUC) measures performance
- Independent of class distribution
Key concepts:
- Perfect classifier: AUC = 1.0
- Random classifier: AUC = 0.5
- Threshold selection
- Model comparison

*For more information, see: [Cross Validation](../5.5-model-eval/cross-validation.md)*

5. What is the purpose of a validation set?
   _a. Tuning hyperparameters_
   b. Final model evaluation
   c. Feature selection
   d. Data preprocessing

**Explanation**: Validation set is used for:
- Hyperparameter tuning
- Model selection
- Preventing overfitting
- Performance estimation during development
Best practices:
- Keep test set separate
- Use cross-validation when data is limited
- Maintain data independence
- Match training data distribution

*For more information, see: [Cross Validation](../5.5-model-eval/cross-validation.md) and [Hyperparameter Tuning](../5.5-model-eval/hyperparameter-tuning.md)*
