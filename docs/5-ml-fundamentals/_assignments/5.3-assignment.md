# Quiz: Supervised Learning Part 2

## Questions

1. What is Random Forest?
   _a. Ensemble of decision trees_
   b. Single optimized tree
   c. Neural network type
   d. Clustering algorithm

**Explanation**: Random Forest:
- Combines multiple decision trees
- Uses bootstrap sampling (bagging)
- Random feature selection at each split
- Aggregates predictions by voting/averaging
Key benefits:
- Reduces overfitting
- Handles high-dimensional data
- Provides feature importance
- Robust to outliers

*For more information, see: [Random Forest](../5.3-supervised-learning-2/random-forest.md)*

2. What is gradient boosting?
   _a. Sequential ensemble learning_
   b. Parallel ensemble learning
   c. Single model optimization
   d. Data preprocessing method

**Explanation**: Gradient Boosting:
- Builds models sequentially
- Each model corrects previous errors
- Uses gradient descent optimization
- Combines weak learners into strong one
Key characteristics:
- High predictive power
- Sensitive to hyperparameters
- Can handle different loss functions
- Prone to overfitting if not tuned

*For more information, see: [Gradient Boosting](../5.3-supervised-learning-2/gradient-boosting.md)*

3. What is a neural network activation function?
   _a. Introduces non-linearity_
   b. Splits data
   c. Measures error
   d. Optimizes weights

**Explanation**: Activation functions:
- Add non-linear transformations
- Enable complex pattern learning
- Control neuron output range
- Affect network learning dynamics
Common functions:
- ReLU (Rectified Linear Unit)
- Sigmoid
- Tanh
- Softmax (for classification)

*For more information, see: [Neural Networks](../5.3-supervised-learning-2/neural-networks.md)*

4. What is backpropagation?
   _a. Algorithm for updating neural network weights_
   b. Data preprocessing step
   c. Model evaluation metric
   d. Feature selection method

**Explanation**: Backpropagation:
- Calculates gradients efficiently
- Propagates error backwards through network
- Updates weights using chain rule
- Core of neural network training
Key concepts:
- Forward pass computation
- Error calculation
- Backward pass gradient computation
- Weight updates using gradients

*For more information, see: [Neural Networks](../5.3-supervised-learning-2/neural-networks.md)*

5. What is dropout in neural networks?
   _a. Regularization technique_
   b. Activation function
   c. Loss function
   d. Optimization algorithm

**Explanation**: Dropout:
- Randomly deactivates neurons during training
- Prevents co-adaptation of neurons
- Reduces overfitting
- Acts as model averaging
Implementation details:
- Different dropout rates for different layers
- Only active during training
- Scaled activations during inference
- Often higher rates in deeper layers

*For more information, see: [Neural Networks](../5.3-supervised-learning-2/neural-networks.md)*
