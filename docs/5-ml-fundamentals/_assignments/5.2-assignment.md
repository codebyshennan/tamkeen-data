# Quiz: Supervised Learning Part 1

## Questions

1. What is Naive Bayes based on?
   _a. Bayes' theorem with independence assumption_
   b. Neural networks
   c. Decision trees
   d. Linear algebra

**Explanation**: Naive Bayes:
- Uses Bayes' theorem for classification
- Assumes feature independence (naive assumption)
- Calculates conditional probabilities
- Works well with high-dimensional data
Key advantages:
- Simple and fast
- Good for text classification
- Handles missing data well
- Works with small training sets

*For more information, see: [Naive Bayes](../5.2-supervised-learning-1/naive-bayes.md)*

2. What is the main idea behind k-Nearest Neighbors?
   _a. Similar instances belong to same class_
   b. Probability calculations
   c. Tree-based decisions
   d. Linear separation

**Explanation**: k-Nearest Neighbors (kNN):
- Classifies based on closest training examples
- Uses distance metrics (e.g., Euclidean)
- Non-parametric and instance-based
- Lazy learning algorithm
Important considerations:
- Choice of k
- Distance metric selection
- Curse of dimensionality
- Feature scaling importance

*For more information, see: [KNN](../5.2-supervised-learning-1/knn.md)*

3. What type of boundary does SVM create?
   _a. Maximum margin hyperplane_
   b. Probability threshold
   c. Tree-based splits
   d. Nearest neighbor regions

**Explanation**: Support Vector Machine (SVM):
- Creates optimal separating hyperplane
- Maximizes margin between classes
- Uses support vectors
- Can handle non-linear boundaries
Key features:
- Kernel trick for non-linearity
- Soft margin for noise tolerance
- Effective in high dimensions
- Good generalization properties

*For more information, see: [SVM](../5.2-supervised-learning-1/svm.md)*

4. What is the advantage of decision trees?
   _a. Easy to interpret and visualize_
   b. Always highest accuracy
   c. Fastest training time
   d. Least memory usage

**Explanation**: Decision trees advantages:
- Clear visual representation
- Natural handling of mixed data types
- Captures non-linear relationships
- Handles missing values well
Key characteristics:
- Hierarchical structure
- Binary or multi-way splits
- Feature importance ranking
- Rule-based decision making

*For more information, see: [Decision Trees](../5.2-supervised-learning-1/decision-trees.md)*

5. Which algorithm is best for text classification?
   _a. Naive Bayes_
   b. KNN
   c. Decision Trees
   d. Linear Regression

**Explanation**: Naive Bayes is ideal for text classification because:
- Handles high-dimensional data well
- Works with sparse feature vectors
- Computationally efficient
- Good performance with small datasets
Specific advantages for text:
- Natural handling of word frequencies
- Fast training and prediction
- Robust to irrelevant features
- Works well with bag-of-words model

*For more information, see: [Naive Bayes](../5.2-supervised-learning-1/naive-bayes.md)*
