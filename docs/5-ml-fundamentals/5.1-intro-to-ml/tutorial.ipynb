{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Introduction to Machine Learning Tutorial\n",
    "\n",
    "This notebook covers key concepts in machine learning including:\n",
    "- What is Machine Learning?\n",
    "- ML Workflow\n",
    "- Feature Engineering\n",
    "- Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Machine Learning?\n",
    "\n",
    "Let's demonstrate the basic concept of machine learning through a simple example of pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate sample data with a pattern\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X.ravel() + 1 + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', alpha=0.5, label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='green', alpha=0.5, label='Testing Data')\n",
    "plt.plot(X_test, y_pred, color='red', label='Predictions')\n",
    "plt.title('Simple Machine Learning Example')\n",
    "plt.xlabel('Input Feature (X)')\n",
    "plt.ylabel('Target Variable (y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model Equation: y = {model.coef_[0]:.2f}x + {model.intercept_:.2f}\")\n",
    "print(f\"True Equation: y = 2x + 1 + noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ML Workflow\n",
    "\n",
    "Let's walk through a typical machine learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Data Collection (simulated)\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = 3*X[:, 0] + 2*X[:, 1] - X[:, 2] + np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Model Training\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "train_pred = model.predict(X_train_scaled)\n",
    "test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, train_pred)\n",
    "test_mse = mean_squared_error(y_test, test_pred)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Testing MSE: {test_mse:.4f}\")\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.scatter(y_train, train_pred, alpha=0.5)\n",
    "ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "ax1.set_title('Training: Predicted vs Actual')\n",
    "ax1.set_xlabel('Actual Values')\n",
    "ax1.set_ylabel('Predicted Values')\n",
    "\n",
    "ax2.scatter(y_test, test_pred, alpha=0.5)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "ax2.set_title('Testing: Predicted vs Actual')\n",
    "ax2.set_xlabel('Actual Values')\n",
    "ax2.set_ylabel('Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Let's explore different feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create sample data with different types of features\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Original features\n",
    "raw_feature1 = np.random.normal(100, 20, n_samples)  # Continuous\n",
    "raw_feature2 = np.random.choice(['A', 'B', 'C'], n_samples)  # Categorical\n",
    "raw_feature3 = np.exp(np.random.normal(0, 1, n_samples))  # Skewed\n",
    "\n",
    "# Feature Engineering Techniques\n",
    "\n",
    "# 1. Standardization\n",
    "standardized = StandardScaler().fit_transform(raw_feature1.reshape(-1, 1))\n",
    "\n",
    "# 2. One-hot Encoding\n",
    "onehot = pd.get_dummies(raw_feature2, prefix='category')\n",
    "\n",
    "# 3. Log Transformation\n",
    "log_transformed = np.log(raw_feature3)\n",
    "\n",
    "# Visualize transformations\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "\n",
    "# Original vs Standardized\n",
    "sns.histplot(raw_feature1, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Original Continuous Feature')\n",
    "\n",
    "sns.histplot(standardized, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Standardized Feature')\n",
    "\n",
    "# Original vs One-hot Encoded\n",
    "sns.countplot(raw_feature2, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Original Categorical Feature')\n",
    "\n",
    "onehot.sum().plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('One-hot Encoded Features')\n",
    "\n",
    "# Original vs Log-transformed\n",
    "sns.histplot(raw_feature3, ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Original Skewed Feature')\n",
    "\n",
    "sns.histplot(log_transformed, ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Log-transformed Feature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bias-Variance Tradeoff\n",
    "\n",
    "Let's demonstrate the bias-variance tradeoff using polynomial regression with different degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Generate data\n",
    "X = np.linspace(0, 1, 30).reshape(-1, 1)\n",
    "y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Create and fit models with different complexities\n",
    "degrees = [1, 3, 15]  # Different polynomial degrees\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, degree in enumerate(degrees, 1):\n",
    "    # Create polynomial pipeline\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Generate smooth predictions for plotting\n",
    "    X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(X, y, color='blue', alpha=0.5, label='Data')\n",
    "    plt.plot(X_test, y_pred, color='red', label=f'Degree {degree}')\n",
    "    plt.title(f'Polynomial Degree {degree}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Degree 1: High bias (underfitting)\")\n",
    "print(\"- Degree 3: Good balance\")\n",
    "print(\"- Degree 15: High variance (overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Create your own dataset and apply different feature engineering techniques.\n",
    "\n",
    "2. Experiment with different data splitting strategies (e.g., different train-test ratios).\n",
    "\n",
    "3. Implement k-fold cross-validation to evaluate model performance.\n",
    "\n",
    "4. Create visualizations to explain the concept of overfitting and underfitting.\n",
    "\n",
    "5. Practice handling missing data and outliers in a machine learning pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
