{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Supervised Learning Part 2 Tutorial\n",
    "\n",
    "This notebook covers advanced supervised learning algorithms including:\n",
    "- Random Forests\n",
    "- Gradient Boosting\n",
    "- Neural Networks\n",
    "- Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forests\n",
    "\n",
    "Let's explore Random Forests and their capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                          n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Random Forest Performance:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': [f'Feature {i+1}' for i in range(X.shape[1])],\n",
    "    'importance': rf.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(10))\n",
    "plt.title('Top 10 Feature Importance in Random Forest')\n",
    "plt.show()\n",
    "\n",
    "# Analyze number of trees\n",
    "n_trees = [10, 50, 100, 200]\n",
    "scores = []\n",
    "\n",
    "for n in n_trees:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    score = cross_val_score(rf, X_train, y_train, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_trees, scores, marker='o')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Cross-validation Score')\n",
    "plt.title('Impact of Number of Trees on Performance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Boosting\n",
    "\n",
    "Let's implement Gradient Boosting and analyze its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Gradient Boosting Performance:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'feature': [f'Feature {i+1}' for i in range(X.shape[1])],\n",
    "    'importance': gb.feature_importances_\n",
    "})\n",
    "feature_importance_gb = feature_importance_gb.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_gb.head(10))\n",
    "plt.title('Top 10 Feature Importance in Gradient Boosting')\n",
    "plt.show()\n",
    "\n",
    "# Analyze learning rate impact\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "scores_lr = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=lr, max_depth=3, random_state=42)\n",
    "    score = cross_val_score(gb, X_train, y_train, cv=5).mean()\n",
    "    scores_lr.append(score)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(learning_rates, scores_lr, marker='o')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Cross-validation Score')\n",
    "plt.title('Impact of Learning Rate on Performance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Networks\n",
    "\n",
    "Let's explore neural networks for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scale features for neural network\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train neural network\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = nn.predict(X_test_scaled)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Neural Network Performance:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(nn.loss_curve_)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Neural Network Learning Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare different architectures\n",
    "architectures = [(50,), (100,), (100, 50), (100, 100)]\n",
    "scores_nn = []\n",
    "\n",
    "for arch in architectures:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=arch, max_iter=1000, random_state=42)\n",
    "    score = cross_val_score(nn, X_train_scaled, y_train, cv=5).mean()\n",
    "    scores_nn.append(score)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(architectures)), scores_nn)\n",
    "plt.xticks(range(len(architectures)), [str(arch) for arch in architectures])\n",
    "plt.xlabel('Network Architecture')\n",
    "plt.ylabel('Cross-validation Score')\n",
    "plt.title('Performance of Different Neural Network Architectures')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Methods\n",
    "\n",
    "Let's combine multiple models using ensemble techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create base models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "\n",
    "# Create voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf), ('gb', gb), ('nn', nn)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Train and evaluate all models\n",
    "models = {\n",
    "    'Random Forest': rf,\n",
    "    'Gradient Boosting': gb,\n",
    "    'Neural Network': nn,\n",
    "    'Ensemble': voting_clf\n",
    "}\n",
    "\n",
    "# Compare model performances\n",
    "model_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    model_scores[name] = score\n",
    "    \n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_scores.keys(), model_scores.values())\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Implement bagging classifier and compare it with random forest.\n",
    "\n",
    "2. Experiment with different neural network activation functions and optimizers.\n",
    "\n",
    "3. Create a stacking ensemble using different base models.\n",
    "\n",
    "4. Analyze the trade-off between model complexity and performance.\n",
    "\n",
    "5. Implement early stopping in gradient boosting and neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
