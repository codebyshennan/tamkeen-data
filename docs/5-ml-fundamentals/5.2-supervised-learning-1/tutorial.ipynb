{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Supervised Learning Part 1 Tutorial\n",
    "\n",
    "This notebook covers fundamental supervised learning algorithms including:\n",
    "- Naive Bayes\n",
    "- k-Nearest Neighbors (KNN)\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes\n",
    "\n",
    "Let's implement and analyze a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, random_state=42, \n",
    "                          n_clusters_per_class=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Naive Bayes Performance:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = 0.02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X, y, nb_model, \"Naive Bayes Decision Boundary\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-Nearest Neighbors (KNN)\n",
    "\n",
    "Let's explore the KNN algorithm and its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate non-linear dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.15, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Try different values of k\n",
    "k_values = [1, 3, 5, 15]\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, k in enumerate(k_values, 1):\n",
    "    # Create and train model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.subplot(1, 4, i)\n",
    "    plot_decision_boundary(X, y, knn, f\"KNN (k={k})\")\n",
    "    \n",
    "    # Print accuracy\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f\"\\nAccuracy for k={k}: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k using cross-validation\n",
    "k_range = range(1, 31)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Plot cross-validation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, cv_scores, 'o-')\n",
    "plt.xlabel('k (number of neighbors)')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.title('Finding Optimal k for KNN')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "optimal_k = k_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nOptimal k: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machines (SVM)\n",
    "\n",
    "Let's implement SVM with different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,\n",
    "                          n_informative=2, random_state=42,\n",
    "                          n_clusters_per_class=1)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Try different kernels\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, kernel in enumerate(kernels, 1):\n",
    "    # Create and train model\n",
    "    svm = SVC(kernel=kernel)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.subplot(1, 3, i)\n",
    "    plot_decision_boundary(X_scaled, y, svm, f\"SVM ({kernel} kernel)\")\n",
    "    \n",
    "    # Print performance metrics\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(f\"\\nPerformance metrics for {kernel} kernel:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Trees\n",
    "\n",
    "Let's explore decision trees and their visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train decision tree\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Decision Tree Performance:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, filled=True, feature_names=[f'Feature {i+1}' for i in range(X.shape[1])])\n",
    "plt.title('Decision Tree Visualization')\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': [f'Feature {i+1}' for i in range(X.shape[1])],\n",
    "    'importance': dt.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance in Decision Tree')\n",
    "plt.show()\n",
    "\n",
    "# Explore effect of tree depth\n",
    "depths = [2, 3, 5, 10]\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, depth in enumerate(depths, 1):\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    plt.subplot(1, 4, i)\n",
    "    plot_decision_boundary(X_scaled, y, dt, f\"Decision Tree (depth={depth})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Compare the performance of all algorithms on a real-world dataset.\n",
    "\n",
    "2. Experiment with different preprocessing techniques and observe their impact.\n",
    "\n",
    "3. Implement grid search for hyperparameter tuning.\n",
    "\n",
    "4. Create your own implementation of KNN from scratch.\n",
    "\n",
    "5. Analyze the impact of feature scaling on SVM performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
