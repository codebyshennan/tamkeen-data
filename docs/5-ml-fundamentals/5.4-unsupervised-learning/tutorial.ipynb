{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Unsupervised Learning Tutorial\n",
    "\n",
    "This notebook covers key unsupervised learning techniques including:\n",
    "- Principal Component Analysis (PCA)\n",
    "- t-SNE\n",
    "- K-means Clustering\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principal Component Analysis (PCA)\n",
    "\n",
    "Let's explore dimensionality reduction using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate high-dimensional data\n",
    "n_samples = 1000\n",
    "n_features = 50\n",
    "n_informative = 5\n",
    "\n",
    "# Create data with only a few informative features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "important_features = np.random.randn(n_features, n_informative)\n",
    "X = np.dot(X, important_features)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot first two principal components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Data Projected onto First Two Principal Components')\n",
    "plt.show()\n",
    "\n",
    "# Print variance explained by first few components\n",
    "print(\"Variance explained by first 5 components:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_[:5], 1):\n",
    "    print(f\"Component {i}: {var:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. t-SNE\n",
    "\n",
    "Let's visualize high-dimensional data using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate data with known clusters\n",
    "X, y = make_blobs(n_samples=500, n_features=10, centers=5, random_state=42)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "plt.title('t-SNE Visualization of High-Dimensional Data')\n",
    "plt.show()\n",
    "\n",
    "# Compare with PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "plt.title('PCA Visualization of High-Dimensional Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-means Clustering\n",
    "\n",
    "Let's explore K-means clustering and its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate clustered data\n",
    "X, true_labels = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Find optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "K = range(1, 10)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, inertias, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Apply K-means with optimal k\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot clustering results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=true_labels, cmap='viridis')\n",
    "plt.title('True Labels')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            marker='x', s=200, linewidths=3, color='r', label='Centroids')\n",
    "plt.title('K-means Clustering')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Clustering\n",
    "\n",
    "Let's implement hierarchical clustering and visualize the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate sample data\n",
    "X, _ = make_blobs(n_samples=50, centers=3, random_state=42)\n",
    "\n",
    "# Create linkage matrix\n",
    "linkage_matrix = linkage(X, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Apply hierarchical clustering\n",
    "n_clusters = 3\n",
    "hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "cluster_labels = hc.fit_predict(X)\n",
    "\n",
    "# Plot clustering results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title('Hierarchical Clustering Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DBSCAN\n",
    "\n",
    "Let's explore density-based clustering using DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate non-spherical clusters\n",
    "X, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
    "\n",
    "# Apply DBSCAN with different parameters\n",
    "eps_values = [0.1, 0.2, 0.3]\n",
    "min_samples_values = [5, 10, 15]\n",
    "\n",
    "fig, axes = plt.subplots(len(eps_values), len(min_samples_values), figsize=(15, 15))\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        # Fit DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = dbscan.fit_predict(X)\n",
    "        \n",
    "        # Plot results\n",
    "        axes[i, j].scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')\n",
    "        axes[i, j].set_title(f'eps={eps}, min_samples={min_samples}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with K-means\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Best DBSCAN parameters\n",
    "best_dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan_labels = best_dbscan.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis')\n",
    "plt.title('K-means Clustering')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis')\n",
    "plt.title('DBSCAN Clustering')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Apply PCA to a real-world dataset and analyze the results.\n",
    "\n",
    "2. Compare t-SNE with UMAP for dimensionality reduction.\n",
    "\n",
    "3. Implement the silhouette score to evaluate clustering quality.\n",
    "\n",
    "4. Try different linkage methods in hierarchical clustering.\n",
    "\n",
    "5. Use DBSCAN to detect outliers in a dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
