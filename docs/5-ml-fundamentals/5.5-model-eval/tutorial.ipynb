{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Model Evaluation Tutorial\n",
    "\n",
    "This notebook covers key concepts in model evaluation including:\n",
    "- Cross-validation\n",
    "- Hyperparameter Tuning\n",
    "- Performance Metrics\n",
    "- Model Selection\n",
    "- Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold,\n",
    "    GridSearchCV, learning_curve, validation_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cross-validation\n",
    "\n",
    "Let's explore different cross-validation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                          n_redundant=5, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Simple K-Fold Cross-validation\n",
    "model = LogisticRegression(random_state=42)\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores:\")\n",
    "print(f\"Individual scores: {cv_scores}\")\n",
    "print(f\"Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Visualize cross-validation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores, 'o-')\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label='Mean CV score')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-validation Scores Across Folds')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X_scaled, y, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Examples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning\n",
    "\n",
    "Let's explore grid search for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Plot grid search results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot scores for different n_estimators\n",
    "plt.subplot(131)\n",
    "sns.boxplot(x='param_n_estimators', y='mean_test_score', data=results)\n",
    "plt.title('Scores vs n_estimators')\n",
    "\n",
    "# Plot scores for different max_depth\n",
    "plt.subplot(132)\n",
    "sns.boxplot(x='param_max_depth', y='mean_test_score', data=results)\n",
    "plt.title('Scores vs max_depth')\n",
    "\n",
    "# Plot scores for different min_samples_split\n",
    "plt.subplot(133)\n",
    "sns.boxplot(x='param_min_samples_split', y='mean_test_score', data=results)\n",
    "plt.title('Scores vs min_samples_split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Metrics\n",
    "\n",
    "Let's analyze different performance metrics for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model with best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection\n",
    "\n",
    "Let's compare different models and their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Compare models using cross-validation\n",
    "model_scores = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    model_scores[name] = scores\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([scores for scores in model_scores.values()], labels=model_scores.keys())\n",
    "plt.title('Model Comparison using Cross-validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "for name, scores in model_scores.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Mean accuracy: {scores.mean():.4f}\")\n",
    "    print(f\"Standard deviation: {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Diagnostics\n",
    "\n",
    "Let's analyze model behavior and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate validation curves\n",
    "param_range = np.logspace(-4, 4, 10)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    LogisticRegression(), X_scaled, y,\n",
    "    param_name=\"C\", param_range=param_range,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std for training scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot validation curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(param_range, train_mean, label=\"Training score\", color=\"darkorange\")\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2,\n",
    "                 color=\"darkorange\")\n",
    "plt.semilogx(param_range, test_mean, label=\"Cross-validation score\", color=\"navy\")\n",
    "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2,\n",
    "                 color=\"navy\")\n",
    "plt.xlabel(\"C parameter\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Validation Curve for Logistic Regression\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze prediction probabilities\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(probabilities[:, 1], bins=50)\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Prediction Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Implement stratified k-fold cross-validation and compare with regular k-fold.\n",
    "\n",
    "2. Use RandomizedSearchCV for hyperparameter tuning and compare with GridSearchCV.\n",
    "\n",
    "3. Implement custom scoring metrics for model evaluation.\n",
    "\n",
    "4. Create a pipeline that includes preprocessing and model training.\n",
    "\n",
    "5. Analyze feature importance and its impact on model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
