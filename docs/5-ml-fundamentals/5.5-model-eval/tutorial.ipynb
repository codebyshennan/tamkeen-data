{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Model Evaluation Tutorial\n",
    "\n",
    "## Welcome to Model Evaluation!\n",
    "\n",
    "Think of model evaluation as being a **quality inspector** for machine learning models. Just like how a car inspector checks brakes, engine, and safety features before approving a vehicle, we need to thoroughly test our models before trusting them with real-world decisions.\n",
    "\n",
    "### What You'll Learn Today\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. **Cross-validation**: Test your model's reliability using multiple \"practice tests\"\n",
    "2. **Hyperparameter Tuning**: Fine-tune your model like adjusting a musical instrument\n",
    "3. **Performance Metrics**: Understand different ways to \"grade\" your model's performance\n",
    "4. **Model Selection**: Choose the best model from multiple candidates\n",
    "5. **Model Diagnostics**: Identify and fix common model problems\n",
    "\n",
    "### Real-World Context\n",
    "\n",
    "Imagine you're building a model to:\n",
    "- **Detect email spam**: Wrong predictions mean important emails in spam folder or spam in inbox\n",
    "- **Diagnose medical conditions**: False negatives could miss serious diseases\n",
    "- **Approve loans**: False positives deny deserving applicants, false negatives approve risky loans\n",
    "\n",
    "The techniques you'll learn help ensure your model performs reliably in these critical situations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Importing Our Tools\n",
    "\n",
    "### What we're doing and why:\n",
    "Before we start building and evaluating models, we need to import the necessary libraries. Think of this like gathering all your tools before starting a home improvement project.\n",
    "\n",
    "**Key libraries we'll use:**\n",
    "- **NumPy & Pandas**: Data manipulation (like Excel for Python)\n",
    "- **Matplotlib & Seaborn**: Creating visualizations (our \"charts and graphs\")\n",
    "- **Scikit-learn**: Machine learning algorithms and evaluation tools\n",
    "\n",
    "**Pro Tip**: Always set a random seed (`np.random.seed(42)`) to make your results reproducible - this ensures you get the same \"random\" numbers each time you run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model selection and evaluation tools\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold, StratifiedKFold,\n",
    "    GridSearchCV, RandomizedSearchCV, learning_curve, validation_curve\n",
    ")\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Dataset generation\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìä Ready to start model evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Cross-Validation: Testing Model Reliability\n",
    "\n",
    "### What is Cross-Validation?\n",
    "\n",
    "**Real-world analogy**: Imagine you're a teacher evaluating a student's performance. You wouldn't judge them based on just one test, right? You'd give multiple tests throughout the semester to get a reliable assessment.\n",
    "\n",
    "Cross-validation does the same thing for machine learning models:\n",
    "- Instead of one train/test split, we create multiple splits\n",
    "- Train and test the model on each split\n",
    "- Average the results to get a more reliable performance estimate\n",
    "\n",
    "### Why Cross-Validation Matters\n",
    "\n",
    "1. **Reduces overfitting**: Prevents the model from \"memorizing\" one specific test set\n",
    "2. **More reliable estimates**: Multiple tests give us confidence in our model's performance\n",
    "3. **Better model comparison**: Fair way to compare different models\n",
    "\n",
    "### What we're doing in this section:\n",
    "1. Create a synthetic dataset (like a practice problem)\n",
    "2. Apply different cross-validation techniques\n",
    "3. Visualize the results to understand model stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a synthetic dataset for practice\n",
    "# What this does: Generates a classification problem with known characteristics\n",
    "print(\"üîß Creating synthetic dataset...\")\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,        # 1000 data points (like 1000 customers)\n",
    "    n_features=20,         # 20 input features (like age, income, etc.)\n",
    "    n_informative=15,      # 15 features actually matter for prediction\n",
    "    n_redundant=5,         # 5 features are combinations of others\n",
    "    random_state=42        # For reproducible results\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset created: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"üéØ Target distribution: {np.bincount(y)} (Class 0: {np.bincount(y)[0]}, Class 1: {np.bincount(y)[1]})\")\n",
    "\n",
    "# Step 2: Scale the features\n",
    "# Why we do this: Many algorithms work better when features are on similar scales\n",
    "print(\"\\n‚öñÔ∏è Scaling features to have mean=0 and std=1...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"‚úÖ Features scaled. Mean: {X_scaled.mean():.3f}, Std: {X_scaled.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple K-Fold Cross-Validation\n",
    "\n",
    "**What we're doing**: Splitting our data into 5 \"folds\" (like 5 different test scenarios) and training/testing our model on each combination.\n",
    "\n",
    "**The process**:\n",
    "1. Split data into 5 equal parts\n",
    "2. Use 4 parts for training, 1 part for testing\n",
    "3. Repeat 5 times, each time using a different part for testing\n",
    "4. Average the 5 test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Perform K-Fold Cross-Validation\n",
    "print(\"üîÑ Performing 5-Fold Cross-Validation...\")\n",
    "print(\"This is like giving our model 5 different practice tests!\\n\")\n",
    "\n",
    "# Create our model (Logistic Regression - a simple, interpretable classifier)\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Perform cross-validation\n",
    "# What this does: Automatically handles the splitting, training, and testing\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"üìä Cross-validation Results:\")\n",
    "print(f\"Individual fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "print(f\"Mean CV score: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard deviation: {cv_scores.std():.4f}\")\n",
    "print(f\"95% Confidence interval: {cv_scores.mean():.4f} ¬± {cv_scores.std() * 2:.4f}\")\n",
    "\n",
    "# Interpret the results\n",
    "print(\"\\nü§î What this means:\")\n",
    "if cv_scores.std() < 0.02:\n",
    "    print(\"‚úÖ Low standard deviation = Consistent performance across folds\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è High standard deviation = Performance varies across folds\")\n",
    "    \n",
    "if cv_scores.mean() > 0.8:\n",
    "    print(\"‚úÖ Good average performance (>80% accuracy)\")\n",
    "elif cv_scores.mean() > 0.7:\n",
    "    print(\"üî∂ Decent performance (70-80% accuracy)\")\n",
    "else:\n",
    "    print(\"‚ùå Poor performance (<70% accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Practice Exercise 1\n",
    "\n",
    "**Your turn!** Try implementing stratified k-fold cross-validation and compare with regular k-fold:\n",
    "\n",
    "```python\n",
    "# Hint: Use StratifiedKFold from sklearn.model_selection\n",
    "# Compare the results with regular KFold\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics: Understanding Your Model's Report Card\n",
    "\n",
    "### What we're doing in this section:\n",
    "1. **Split data** into train/test sets\n",
    "2. **Train our best model** from hyperparameter tuning\n",
    "3. **Calculate multiple metrics** to get a complete picture\n",
    "4. **Visualize results** with confusion matrix, ROC curve, and precision-recall curve\n",
    "\n",
    "### Why Multiple Metrics Matter\n",
    "\n",
    "**Real-world analogy**: Evaluating a model with just accuracy is like judging a restaurant with only one review. You need multiple perspectives:\n",
    "- **Accuracy**: Overall correctness (like overall rating)\n",
    "- **Precision**: Quality of positive predictions (like food quality)\n",
    "- **Recall**: Completeness of positive detection (like service quality)\n",
    "- **F1-Score**: Balance between precision and recall (like value for money)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for final evaluation\n",
    "print(\"üî™ Splitting data for final evaluation...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Test set class distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Train our best model (using Random Forest with default parameters for now)\n",
    "print(\"\\nüå≤ Training Random Forest model...\")\n",
    "best_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "print(\"‚úÖ Model trained and predictions made!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Metrics Analysis\n",
    "\n",
    "**What each metric tells us:**\n",
    "- **Accuracy**: What percentage of predictions were correct?\n",
    "- **Precision**: Of all positive predictions, how many were actually positive?\n",
    "- **Recall**: Of all actual positives, how many did we correctly identify?\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "print(\"üìä Performance Metrics Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.1f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.1f}%)\")\n",
    "print(f\"F1-Score:  {f1:.4f} ({f1*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüîç What these numbers mean:\")\n",
    "print(f\"‚Ä¢ Out of 100 predictions, {accuracy*100:.0f} are correct\")\n",
    "print(f\"‚Ä¢ Out of 100 positive predictions, {precision*100:.0f} are actually positive\")\n",
    "print(f\"‚Ä¢ Out of 100 actual positives, we catch {recall*100:.0f} of them\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix: Where Did We Go Wrong?\n",
    "\n",
    "**What we're visualizing**: A table showing correct and incorrect predictions for each class.\n",
    "\n",
    "**How to read it**:\n",
    "- **Diagonal elements**: Correct predictions\n",
    "- **Off-diagonal elements**: Mistakes\n",
    "- **Darker colors**: Higher numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted: Class 0', 'Predicted: Class 1'],\n",
    "            yticklabels=['Actual: Class 0', 'Actual: Class 1'])\n",
    "plt.title('Confusion Matrix: Model Predictions vs Reality', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Add interpretation text\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "plt.text(0.5, -0.15, \n",
    "         f'True Negatives: {tn}  |  False Positives: {fp}\\n' +\n",
    "         f'False Negatives: {fn}  |  True Positives: {tp}',\n",
    "         transform=plt.gca().transAxes, ha='center', fontsize=11,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze the confusion matrix\n",
    "print(\"üîç Confusion Matrix Analysis:\")\n",
    "print(f\"True Negatives (TN): {tn} - Correctly identified negative cases\")\n",
    "print(f\"False Positives (FP): {fp} - Incorrectly labeled as positive (Type I Error)\")\n",
    "print(f\"False Negatives (FN): {fn} - Missed positive cases (Type II Error)\")\n",
    "print(f\"True Positives (TP): {tp} - Correctly identified positive cases\")\n",
    "\n",
    "if fp > fn:\n",
    "    print(\"\\n‚ö†Ô∏è More false positives than false negatives - model is aggressive\")\n",
    "elif fn > fp:\n",
    "    print(\"\\n‚ö†Ô∏è More false negatives than false positives - model is conservative\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Balanced error types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Practice Exercise 2\n",
    "\n",
    "**Your turn!** Create a pipeline that includes preprocessing and model training, then evaluate it:\n",
    "\n",
    "```python\n",
    "# Hint: Use Pipeline from sklearn.pipeline\n",
    "# Include StandardScaler and your chosen classifier\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### üéâ Congratulations! You've learned:\n",
    "\n",
    "1. **Cross-validation**: How to reliably test model performance\n",
    "2. **Hyperparameter tuning**: How to optimize model settings\n",
    "3. **Performance metrics**: How to comprehensively evaluate models\n",
    "4. **Model selection**: How to choose the best model\n",
    "5. **Model diagnostics**: How to identify and fix problems\n",
    "\n",
    "### üöÄ What's Next?\n",
    "\n",
    "1. **Practice with real datasets**: Apply these techniques to actual problems\n",
    "2. **Learn advanced techniques**: Nested cross-validation, custom metrics\n",
    "3. **Explore other algorithms**: Try different models and compare them\n",
    "4. **Study domain-specific evaluation**: Learn evaluation techniques for your field\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "- **Always use cross-validation** for reliable performance estimates\n",
    "- **Tune hyperparameters** to get the best performance\n",
    "- **Use multiple metrics** to get a complete picture\n",
    "- **Visualize results** to understand model behavior\n",
    "- **Consider business context** when choosing metrics and thresholds\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Model Evaluation Best Practices](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Cross-validation Documentation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "**Remember**: Model evaluation is not just about getting high scores - it's about building reliable, trustworthy models that work well in the real world!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
