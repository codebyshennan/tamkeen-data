# Cross Validation

## What is Cross Validation? ðŸ¤”

Imagine you're a chef testing a new recipe. Instead of just cooking it once, you'd want to try it multiple times with different ingredients and conditions to make sure it works well in various situations. Cross validation in machine learning works the same way! It helps us test our models multiple times to ensure they perform well in different scenarios.

### Why Cross Validation Matters ðŸŒŸ

Think of cross validation like a student taking multiple practice tests before the final exam. It helps us:

1. Get a more reliable estimate of how well our model will perform
2. Catch if our model is "memorizing" the data (overfitting) instead of learning patterns
3. Compare different models fairly
4. Make sure our model is stable and reliable

## Real-World Analogies ðŸ“š

### The Restaurant Menu Analogy

Imagine you're opening a new restaurant. You wouldn't just serve your menu to one group of customers and call it a success. Instead, you'd:

- Test different dishes with various groups of customers
- Get feedback from different demographics
- Try different times of day
- Consider different seasons

This is exactly what cross validation does for machine learning models!

### The Sports Team Analogy

Think of cross validation like a sports team's practice games:

- Each fold is like a practice game
- The training data is like your team's practice
- The validation data is like the practice game
- The final model is like your team going into the real season

## Types of Cross Validation ðŸŽ¯

### 1. K-Fold Cross Validation

This is like dividing your data into K equal parts and testing your model K times, each time using a different part as the test set.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

# Create sample dataset
X, y = make_classification(n_samples=1000, n_features=20, 
                         n_informative=15, n_redundant=5,
                         random_state=42)

# Perform cross-validation
model = LogisticRegression()
scores = cross_val_score(model, X, y, cv=5)

print(f"Cross-validation scores: {scores}")
print(f"Mean CV score: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
```

### 2. Stratified K-Fold

This is like making sure each practice game has the same mix of players as your full team. It maintains the same proportion of classes in each fold.

```python
from sklearn.model_selection import StratifiedKFold

def compare_fold_distributions(X, y, n_splits=5):
    # Regular K-Fold
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    # Stratified K-Fold
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
    
    # Plot regular K-Fold distributions
    for i, (_, val_idx) in enumerate(kf.split(X)):
        ax1.bar(i, y[val_idx].mean(), label=f'Fold {i+1}')
    ax1.axhline(y.mean(), color='r', linestyle='--', label='Overall mean')
    ax1.set_title('Regular K-Fold Class Distribution')
    ax1.legend()
    
    # Plot Stratified K-Fold distributions
    for i, (_, val_idx) in enumerate(skf.split(X, y)):
        ax2.bar(i, y[val_idx].mean(), label=f'Fold {i+1}')
    ax2.axhline(y.mean(), color='r', linestyle='--', label='Overall mean')
    ax2.set_title('Stratified K-Fold Class Distribution')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('assets/stratified_vs_regular_kfold.png')
    plt.show()

compare_fold_distributions(X, y)
```

### 3. Time Series Cross Validation

This is like testing a weather prediction model by using past data to predict future weather. We can't use future data to predict the past!

```python
from sklearn.model_selection import TimeSeriesSplit

def visualize_timeseries_cv(X, n_splits=5):
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    plt.figure(figsize=(15, n_splits*2))
    for i, (train_idx, val_idx) in enumerate(tscv.split(X)):
        plt.subplot(n_splits, 1, i+1)
        plt.scatter(range(len(X)), [i]*len(X), c='lightgray', alpha=0.5)
        plt.scatter(train_idx, [i]*len(train_idx), c='blue', label='Train')
        plt.scatter(val_idx, [i]*len(val_idx), c='red', label='Validation')
        plt.title(f'Time Series Split {i+1}')
        if i == 0:
            plt.legend()
    
    plt.tight_layout()
    plt.savefig('assets/timeseries_cv.png')
    plt.show()

# Visualize Time Series splits
visualize_timeseries_cv(X)
```

## Common Mistakes to Avoid âš ï¸

1. **Using the Wrong Type of Cross Validation**
   - Using regular K-fold for imbalanced data
   - Using time series CV for independent data
   - Using leave-one-out for large datasets

2. **Data Leakage**
   - Scaling features before cross-validation
   - Feature selection before splitting
   - Using future information to predict the past

3. **Insufficient Folds**
   - Using too few folds for small datasets
   - Using too many folds for large datasets
   - Not considering computational cost

## Practical Example: Credit Risk Prediction ðŸ’³

Let's see how cross validation helps in a real-world scenario:

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Create credit risk dataset
np.random.seed(42)
n_samples = 1000

# Generate features
age = np.random.normal(35, 10, n_samples)
income = np.random.exponential(50000, n_samples)
credit_score = np.random.normal(700, 100, n_samples)

X = np.column_stack([age, income, credit_score])
y = (credit_score + income/1000 + age > 800).astype(int)  # Binary target

# Create pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier())
])

# Perform stratified cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores = []
for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    pipeline.fit(X_train, y_train)
    score = pipeline.score(X_val, y_val)
    scores.append(score)
    print(f"Fold {fold+1}: {score:.3f}")

print(f"\nMean CV score: {np.mean(scores):.3f} (+/- {np.std(scores) * 2:.3f})")
```

## Best Practices ðŸŒŸ

### 1. Choosing the Right Number of Folds

```python
def choose_optimal_k(X, y, k_range=range(2, 11)):
    scores = []
    stds = []
    
    for k in k_range:
        cv_scores = cross_val_score(
            LogisticRegression(),
            X, y,
            cv=k
        )
        scores.append(cv_scores.mean())
        stds.append(cv_scores.std())
    
    plt.figure(figsize=(10, 5))
    plt.errorbar(k_range, scores, yerr=stds, fmt='o-')
    plt.xlabel('Number of Folds')
    plt.ylabel('Cross-validation Score')
    plt.title('Impact of K on Cross-validation')
    plt.grid(True)
    plt.savefig('assets/optimal_k_selection.png')
    plt.show()

choose_optimal_k(X, y)
```

## Additional Resources ðŸ“š

1. **Online Courses**
   - Coursera: Machine Learning by Andrew Ng
   - edX: Introduction to Machine Learning

2. **Books**
   - "Introduction to Machine Learning with Python" by Andreas MÃ¼ller
   - "Hands-On Machine Learning with Scikit-Learn" by AurÃ©lien GÃ©ron

3. **Documentation**
   - [Scikit-learn Cross Validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)
   - [Time Series Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)

## Next Steps ðŸš€

Ready to learn more? Check out:

1. [Hyperparameter Tuning](./hyperparameter-tuning.md) to optimize your model's performance
2. [Model Metrics](./metrics.md) to understand different ways to evaluate your model
3. [Model Selection](./model-selection.md) to choose the best model for your problem
